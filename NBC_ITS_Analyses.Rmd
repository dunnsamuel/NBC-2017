---
title: "NBC ITS Analyses"
author: "Sam Dunn"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: 
    fig_height: 8
    fig_width: 8
    toc: yes
---
This workthrough uses the mothur output file egenrated by following the mothur  MiSeq SOP. 


Load data from the mothur outptu file and load packages needed for data manipulation, analysis,and visulaization.
```{r Load Data, echo=FALSE,include=FALSE,warning=FALSE}

its.df<-read.delim(file="nbcits.final.an.0.01.cons.tax.summary")
#its.df=its.df[-which(its.df$taxon=="unknown"),]
# ipak function: install and load multiple R packages.
# check to see if packages are installed. Install them if they are not, then load them into the R session.

ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE, repos='http://cran.us.r-project.org')
    sapply(pkg, require, character.only = TRUE)
}

# usage
packages <- c("ggplot2", 
              "plyr", 
              "reshape2", 
              "RColorBrewer", 
              "scales", 
              "grid",
              "VennDiagram",
              "vegan",
              "RAM",
              "tidyr",
              "Rmisc",
              "reshape","reshape2",
              "gapminder","magrittr","dplyr","ggpubr","gridExtra","
              patternplot","tibble","gplots","broom","data.table","nlme","devtools","bibtex")
ipak(packages)

write.bib(packages, file="Rpackages.nbc.bib",append=F,verbose=T)
install_github("pmartinezarbizu/pairwiseAdonis/pairwiseAdonis")
library(pairwiseAdonis)
write.bib(citation(),file="R_build.bib",append=F,verbose=T)

theme_set(theme_gray(base_size = 18))
pd=position_dodge(width=0.4)

#design.file.23=as.data.frame(colnames(its.df))
#design.file.23=as.data.frame(design.file.23)
#design.file.23=t(design.file.23)
#design.file.23=as.data.frame(design.file.23)

#design.file.23=separate(design.file.23,'colnames(its.df)',into=c("x","Group"),sep=c(1))
#design.file.23$x=NULL
#design.file.23=separate(design.file.23,Group,into=c("Date","Substrate"),sep=c(6),remove=FALSE)
#design.file.23=separate(design.file.23,Substrate,into=c("Substrate","Rep"),sep=c(-1))
#write.table(design.file,file="NBC_design_23.txt",sep="\t")



```


We are interested in the effects of time and substrate on the community we sequenced.  Shannon Doversity is a good first metric.  We need to select only 1 taxa level using the "taxlevel" column in our dataset and then perform the analysis.
```{r Shannon Diversity,echo=FALSE}
its.df$taxlevel=as.factor(its.df$taxlevel)
its.df.6=subset.data.frame(its.df, taxlevel==6,drop=TRUE)

#you'll have to update your first and last sample column name in the "vars" argument below.  You will also need to look at your sample (column) names and adjust the arguments in the separate function.  Briefly, change the arguments int he "into" list to represent the number o chunks you can split your sample name into.  Secondly,the "sep"  says how many places fromt he left or right(-) the cuts should be made to separate.  The number of values in sep should each the number of groups in into less 1.
its.shannon.df<-its.df.6 %>% 
  subset(select=c(-taxlevel,-rankID,-taxon,-daughterlevels,-total,-SDKC2,-SDKC3)) %>% 
  mutate_at(vars(X170818FA:X170915TC),funs(diversity(.,index="shannon"))) %>%
  gather(key="Sample",value="Diversity") %>%
  distinct(Sample,Diversity) %>% 
  separate(Sample, into=c("junk","Date","Substrate","Rep"), sep=c(1,7,-1))
  

#Clean up time.  First we remove the junk column altogether.  seconldy, we do some cleaning in case controls made it in.  Lastly we want to convert our "date" code into a "day of incuation" which we do manually with the revalue function.

its.shannon.df$junk=NULL
its.shannon.df=its.shannon.df[-60,]
its.shannon.df=droplevels(its.shannon.df[-which(its.shannon.df$Substrate=='CR'),]) #drop controls
its.shannon.df$Date=as.factor(its.shannon.df$Date)
its.shannon.df$Date=revalue(its.shannon.df$Date,c("170818"="3","170828"="10","170905"="21","170915"="31"))
its.shannon.df$Substrate=factor(its.shannon.df$Substrate,levels=c("H","SO","SH","F","T"),labels=c("PVC","Soft PE","Sheet PE","PS","Tile"))


shannon.div.summary<-its.shannon.df %>% 
  group_by(Date, Substrate) %>% 
  summarise(mean=mean(Diversity),se=(sd(Diversity)/sqrt(n())))


#figure
shannon.div.its=ggplot(shannon.div.summary, aes(x=Date,y=mean,group=Substrate,shape=Substrate,fill=Substrate))+
  geom_point(position=pd,size=4)+geom_line(aes(linetype=Substrate),position=pd)+
  geom_errorbar(aes(ymin=mean-se,ymax=mean+se),width=0.3,position=pd)+
  ylab("Shannon Diversity (H')")+
  theme_classic()+
  theme(legend.position=c(0.25,0.95),
        legend.title=element_blank(),
        legend.direction="horizontal",
        axis.text=element_text(size=16),
        axis.title=element_text(size=16),
        legend.text=element_text(size=14))+
  scale_shape_manual(name="Substrate",values=c(21,22,23,24,25,26))+
  ggtitle("Fungal Communities")+
  scale_y_continuous(breaks=seq(0,5,0.5))
shannon.div.its


its.shan.aov<-tidy(aov(Diversity~Date+Substrate+Date:Substrate,data=its.shannon.df))

shannon.div.its.bw=ggplot(shannon.div.summary, aes(x=Date,y=mean,group=Substrate,shape=Substrate))+
  geom_point(position=pd,size=4,fill="black")+
  geom_line(aes(linetype=Substrate),position=pd)+
  geom_errorbar(aes(ymin=mean-se,ymax=mean+se),width=0.3,position=pd)+
  ylab("Shannon Diversity (H')")+
  theme_classic()+
  theme(legend.position=c(0.25,0.95),
        legend.title=element_blank(),
        legend.direction="horizontal",
        axis.text=element_text(size=16),
        axis.title=element_text(size=16),
        legend.text=element_text(size=14))+
  scale_shape_manual(name="Substrate",values=c(21,22,23,24,25,26))+
  ggtitle("Fungal Communities")+
  scale_y_continuous(breaks=seq(0,5,0.5))
shannon.div.its.bw




saveRDS(its.shannon.df,"its.shannon.Rdata")
```


We want to count the number of unqiue taxa we have ine ach sample (or OTUs, operational taxonomic units).  TO do this we use a simple custom function and look for values greater than 0 in each row of the dataset
```{r OTU Richness,include=FALSE}
its.df$taxlevel=as.factor(its.df$taxlevel)
its.df.6=subset.data.frame(its.df, taxlevel==6,drop=TRUE)
observationThreshold = 1 #set min number to count as OTU being present

otu=apply(its.df.6[,6:68]>=observationThreshold, 2, sum,na.rm=T) #check by row to see if each taxa present or not
#otu.0=apply(its.df.6[,6:65],2,sum,na.rm=T)
drops=c("SDKC1","SDKC2","SDKC3")
#otu=otu[,!(names(its.df.6)%in% drops)]


list_to_df <- function(list_for_df) {
  list_for_df <- as.list(list_for_df)

  nm <- names(list_for_df)
  if (is.null(nm))
    nm <- seq_along(list_for_df)

  df <- data.frame(name = nm, stringsAsFactors = FALSE)
  df$value <- unname(list_for_df)
  df
}

otu.df=list_to_df(otu) #conevrt list to data frame wiht custom function above
otu.df$value=as.numeric(otu.df$value)
otu.df=separate(otu.df,name,into=c("junk","Date","Substrate"),sep=c(1,7))
otu.df=separate(otu.df,Substrate,into=c("Substrate","Rep"),sep=c(-1))
otu.df$junk=NULL
otu.df=otu.df[-60,]
otu.df=droplevels(otu.df[-which(otu.df$Substrate=='CR'),])
otu.df$Date=as.factor(otu.df$Date)
otu.df$Date=revalue(otu.df$Date,c("170818"="3","170828"="10","170905"="21","170915"="31"))

out.aov=otu.df%>% #tidy pipes!  Run anovs by Date on otu richenss
  group_by(Date)%>%
  do(tidy(aov(value~Substrate,data=.)))
out.aov

tukey.out=otu.df%>%
  group_by(Date)%>%
  do(multitst=TukeyHSD(aov(value~Substrate,data=.)))
tukey.out %>%tidy(multitst)



otu.df.summary=summarySE(otu.df,measurevar="value",groupvars=c("Date","Substrate"),na.rm=TRUE)

pd=position_dodge(width=0.4)
otu.plot=ggplot(otu.df.summary, aes(x=Date,y=value,group=Substrate,shape=Substrate,fill=Substrate))+
  geom_point(position=pd,size=4)+geom_line(aes(linetype=Substrate),position=pd)+
  geom_errorbar(aes(ymin=value-se,ymax=value+se),width=0.3,position=pd)+
  ylab("Observed OTUs")+
  theme_classic()+
  theme(legend.position=c(0.25,0.95),
        legend.title=element_blank(),
        legend.direction="horizontal",
        axis.text=element_text(size=16),
        axis.title=element_text(size=16),
        legend.text=element_text(size=14))+
  scale_shape_manual(name="Substrate",values=c(21,22,23,24,25,26))
otu.plot
```

Absolute Abundance is only so useful.  Here we calculate relatiev abundances for each taxa for its sample and plot stacked bars and a heatmap.
```{r Relative Abundance Stacked Bar plots,echo=FALSE}

theme_set(theme_classic(base_size = 18))
its.df.3=subset.data.frame(its.df, taxlevel==3,drop=TRUE) #taxlevel 3 is class

drops=c("SDKC1","SDKC2","SDKC3")
its.df.3=its.df.3[,!(names(its.df.3)%in% drops)]

sites=list(colnames(its.df.3[,6:72])) #get list of samples
total=list(colnames(its.df.3[,1]))

#custome relative abudnace function/  it takes a list of sites and the total column for each taxa and finds the relaive abundance of each taxa in each sample
rabund=function(sites){
  rabund=sites/its.df.3$total
  return(rabund)
}


rabund.df=rabund(its.df.3[,6:72])
rabund.df=cbind(its.df.3$taxon,rabund.df)


#dcast and melt are the modern equivalents of transposition.  they work really well until they don't.
rabund.df=dcast(melt(rabund.df,id.vars="its.df.3$taxon"),variable~its.df.3$taxon)
rabund.df=separate(rabund.df,variable,into=c("junk","Date","Substrate"),sep=c(1,7))
rabund.df=separate(rabund.df,Substrate,into=c("Substrate","Rep"),sep=c(-1))
rabund.df$junk=NULL

rabund.df=droplevels(rabund.df[-which(rabund.df$Substrate=='CR'),])
rabund.df$Date=as.factor(rabund.df$Date)
rabund.df$Date=revalue(rabund.df$Date,c("170818"="3","170828"="10","170905"="21","170915"="31"))

rabund.h=aggregate(rabund.df[,4:30],list(rabund.df$Date,rabund.df$Substrate),mean)
taxa=list(colnames(rabund.h[,4:29])) 
rabund.h=melt(rabund.h,id.vars=c("Group.1","Group.2"))

  

#I couldn't make this automated so I did it manually....don't hate me

rabund.1=filter(rabund.df, Date==3)
rabund.1$Date=NULL
rabund.1=aggregate(rabund.1[,3:29],list(rabund.1$Substrate),mean)
rabund.1=melt(rabund.1, id="Group.1")
rabund.1$Group.1=as.factor(rabund.1$Group.1)


rabund.2=filter(rabund.df,Date==10)
rabund.2$Date=NULL
rabund.2=aggregate(rabund.2[,3:29],list(rabund.2$Substrate),mean)
rabund.2=melt(rabund.2, id="Group.1")


rabund.3=filter(rabund.df, Date==21)
rabund.3$Date=NULL
rabund.3=aggregate(rabund.3[,3:29],list(rabund.3$Substrate),mean)
rabund.3=melt(rabund.3, id="Group.1")


rabund.4=filter(rabund.df, Date==31)
rabund.4$Date=NULL
rabund.4=aggregate(rabund.4[,3:29],list(rabund.4$Substrate),mean)
rabund.4=melt(rabund.4, id="Group.1")




#select top 40% by substrate
rabund.1.10=rabund.1%>%
  group_by(Group.1)%>%
  arrange(Group.1,desc(value))%>%
  slice(seq(n()*.4)) #here is where you can select your cutoff level
rabund.1.sigma=dcast(rabund.1.10,Group.1~variable)
rabund.1.sigma=t(rabund.1.sigma)
write.table(rabund.1.sigma,file="170818_rabund.txt",sep="\t")

rabund.2.10=rabund.2%>%
  group_by(Group.1)%>%
  arrange(Group.1,desc(value))%>%
  slice(seq(n()*.4))
rabund.2.sigma=dcast(rabund.2.10,Group.1~variable)
rabund.2.sigma=t(rabund.2.sigma)
write.table(rabund.2.sigma,file="170828_rabund.txt",sep="\t")

rabund.3.10=rabund.3%>%
  group_by(Group.1)%>%
  arrange(Group.1,desc(value))%>%
  slice(seq(n()*.4))
rabund.3.sigma=dcast(rabund.3.10,Group.1~variable)
rabund.3.sigma=t(rabund.3.sigma)
write.table(rabund.3.sigma,file="170905_rabund.txt",sep="\t")

rabund.4.10=rabund.4%>%
  group_by(Group.1)%>%
  arrange(Group.1,desc(value))%>%
  slice(seq(n()*.4))
rabund.4.sigma=dcast(rabund.4.10,Group.1~variable)
rabund.4.sigma=t(rabund.4.sigma)
write.table(rabund.4.sigma,file="170915_rabund.txt",sep="\t")

  


stack.23s.1=ggplot(rabund.1.10, aes(x=Group.1,y=value,group=variable,color=variable,fill=variable))+
  geom_bar(stat="identity",color="black")+
  ggtitle("170818")+
  ylab("Relative Abundance")+
  xlab("Substrate")+
  theme_classic()
stack.23s.1
cat("\n")
stack.23s.2=ggplot(rabund.2.10, aes(x=Group.1,y=value,group=variable,color=variable,fill=variable))+
  geom_bar(stat="identity",color="black")+
  ggtitle("170828")+
  ylab("Relative Abundance")+
  xlab("Substrate")+
  theme_classic()
stack.23s.2
cat("\n")
stack.23s.3=ggplot(rabund.3.10, aes(x=Group.1,y=value,group=variable,color=variable,fill=variable))+
  geom_bar(stat="identity",color="black")+
  ggtitle("170905")+
  ylab("Relative Abundance")+
  xlab("Substrate")+
  theme_classic()
stack.23s.3
cat("\n")
stack.23s.4=ggplot(rabund.4.10, aes(x=Group.1,y=value,group=variable,color=variable,fill=variable))+
  geom_bar(stat="identity",color="black")+
  ggtitle("170915")+
  ylab("Relative Abundance")+
  xlab("Substrate")+
  theme_classic()
stack.23s.4
#ggarrange(stack.23s.1,stack.23s.2,stack.23s.3,stack.23s.4,ncol=2,nrow=2,labels="auto",legend="right", font.label = list(size = 18, color = "black"))
```
Stacked bars are not always the best for visualzing communities.  I prefer heatmaps personally and figured out how to make a facetted heatmap.  if you're not familiar with facet_grid() in ggplot it will rock your world.
```{r Heatmap Plot,echo=FALSE}
its.df.3=subset.data.frame(its.df, taxlevel==3,drop=TRUE) #Note the higher tax level...purely from a visualization standpoint
its.df.3$taxlevel=NULL
its.df.3$rankID=NULL
its.df.3$daughterlevels=NULL
its.df.3$total=NULL
its.rab<-its.df.3 %>% 
  gather(-SDKC2,-SDKC3,-taxon,key="Sample",value="obs") 
  
its.rab<-its.rab %>%   
  group_by(Sample) %>% 
  mutate(total_obs=sum(obs),
         rabund=obs/total_obs)



its.rab<-separate(its.rab,Sample, into=c("Junk","Date","Substrate","Rep"),sep=c(1,7,-1)) 
its.rab$Junk=NULL

rabund.df=droplevels(its.rab[-which(its.rab$Substrate=='CR'),])
rabund.df$Date=as.factor(rabund.df$Date)
rabund.df$Date=revalue(rabund.df$Date,c("170818"="3","170828"="10","170905"="21","170915"="31"))
rabund.df$Substrate=factor(rabund.df$Substrate,levels=c("H","SO","SH","F","T"),labels=c("PVC","Soft PE","Sheet PE","PS","Tile"))

rabund.sum.df<-rabund.df %>% 
  group_by(Date,Substrate,taxon) %>% 
  summarise(mean_rabund=mean(rabund),
            mean_obs=mean(obs))



require(viridis) #viridis is a package of alternative color scales to allow for high contrast observations....i.e. whent here are very subtle changes.
ggplot(rabund.sum.df, aes(Date,taxon,fill=mean_rabund))+
  geom_tile(width=1)+
  facet_wrap(~Substrate,nrow=1)+#facet wrap splits our graph into multiuple chunks where each has a unique valeu for some factor
  scale_fill_viridis_c(name="Root Mean Relative\nAbundance")+  #\n creates a line break in text
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=10),
        strip.text=element_text(size=10),
        legend.title = element_text(size=12))+
  ylab("Class")+
  xlab("Day of Incubation")
  
ggsave("its.hm.jpeg", device="jpeg", width=8, height=10,units="in")
#you can also set dimensions when your print your figures
```

These are all pretty graphs so far, but you haven't answered the question!  Are there differences between substrates, dates, and their interaction?!  Never fear we will answer that int he following code chunk using ANOSIM (analysis of similirity).  ANOSIM is a multivairate method that is similar to ANOVA, but it instead uses a disimilarity matrix (bray curtis in oru case) instead of the data directly.

What's better is that ANOSIM is fully compatible with NMDS plots (the following code chunk)  So we can quantify differences between our treatment combinations and then plot them cleanly using NMDS!

```{r ANOSIM,echo=FALSE,warning=FALSE}
#https://sites.google.com/site/mb3gustame/hypothesis-tests/anosim
its.df.6=subset.data.frame(its.df, taxlevel==6,drop=TRUE)
drops=c("taxlevel","rankID","daughterlevels","total")
its.df.6=its.df.6[,!(names(its.df.6)%in% drops)]
anosim.df=t(its.df.6)
anosim.df=as.data.frame(anosim.df)
anosim.df[]=lapply(anosim.df,as.character)
colnames(anosim.df)=anosim.df[1,]
anosim.df=anosim.df[-1,]
anosim.df=anosim.df%>%
  rownames_to_column
anosim.df[,2:295]=lapply(anosim.df[,2:295],function(x) as.numeric(as.character(x)))





anosim.df=separate(anosim.df,rowname,into=c("junk","Date","Substrate"),sep=c(1,7))
anosim.df=separate(anosim.df,Substrate,into=c("Substrate","Rep"),sep=c(-1))
anosim.df$junk=NULL

anosim.df=droplevels(anosim.df[-which(anosim.df$Substrate=='CR'),])
anosim.df$Date=as.factor(anosim.df$Date)
anosim.df$Substrate=as.factor(anosim.df$Substrate)
anosim.df$Date=revalue(anosim.df$Date,c("170818"="3","170828"="10","170905"="21","170915"="31"))




anosim.3=subset(anosim.df, anosim.df$Date=="3")
anosim.10=subset(anosim.df, anosim.df$Date=="10")
anosim.21=subset(anosim.df, anosim.df$Date=="21")
anosim.31=subset(anosim.df, anosim.df$Date=="31")
anosim.df=anosim.df[,1:60]
#anosim.31=anosim.31[,colSums(anosim.31 !=0)>0]
#I wanted to try runnign ANOSIM for each date but there is not great support for this.  so I (gasp) subsetted my data (don't hate me)


design.df=anosim.df[,1:3] #design file contains our sample, treatment rep info.  ANOSIM requires a matrix, not a dataframer
design.3=anosim.3[,1:3]
design.10=anosim.3[,1:3]
design.21=anosim.3[,1:3]
design.31=anosim.3[,1:3]

anosim.df$Date=NULL
anosim.df$Substrate=NULL
anosim.df$Rep=NULL

anosim.3$Date=NULL
anosim.3$Substrate=NULL
anosim.3$Rep=NULL

anosim.10$Date=NULL
anosim.10$Substrate=NULL
anosim.10$Rep=NULL

anosim.21$Date=NULL
anosim.21$Substrate=NULL
anosim.21$Rep=NULL

anosim.31$Date=NULL
anosim.31$Substrate=NULL
anosim.31$Rep=NULL

#its.ano=adonis(anosim.df~Substrate*Date,data=design.df) #sig  #adonis is the call for anosim n {vegan}

its.perm=print(adonis(anosim.df~Substrate*Date,data=design.df))
its.perm.tab=as.data.frame(its.perm$aov.tab)
its.perm.tab=its.perm.tab[1:3,]

adonis(anosim.3~Substrate,data=design.3) #ns
adonis(anosim.10~Substrate,data=design.10) #ns
adonis(anosim.21~Substrate,data=design.21) #ns
adonis(anosim.31~Substrate,data=design.31) #sig

#pairwise.adonis(anosim.3,factors=design.3$Substrate, #foudn this and adapted it to my code.  it more or less agrees with my multiple samplign approach above.  Pvalues are different because of bonferroni but the results dont really change
  #              sim.method = 'bray', 
 #               p.adjust.m ='bonferroni')
#pairwise.adonis(anosim.10,factors=design.10$Substrate, 
  #              sim.method = 'bray', 
 #               p.adjust.m ='bonferroni')
#pairwise.adonis(anosim.21,factors=design.21$Substrate, 
    #            sim.method = 'bray', 
   #             p.adjust.m ='bonferroni')
pairwise.adonis(anosim.31,factors=design.31$Substrate, 
                sim.method = 'bray', 
                p.adjust.m ='bonferroni')
#pairwise.adonis(anosim.df,factors=design.df$Substrate, 
 #               sim.method = 'bray', 
  #              p.adjust.m ='bonferroni')
```


So we know whetehr our sample communities differ or not, but now we want to explain these differences (or lack thereof).  How do we bring our non-community environmental data?  We will use the envfit function in vegan to do this!  Envfit takes your community distance matrices and looks for correlations with a similarlys tructured dataset.  Your data sets (community and env) need to be the same size and not contain missing values for this to work.
```{r Envfit,echo=FALSE}
env.df <- read.csv("env.df.csv") #external data for vector fitting (laba ssays, field data,etc)
env.drops=c("8/15/2017",
            "8/22/2017",
            "8/25/2017",
            "9/1/2017",
            "9/8/2017",
            "9/12/2017") #beware of date format shifts in excel!!  I am dropping dates from the env.df because I didn't get community measurements for each date


env.df=droplevels(env.df[!(env.df$Date.Sampled %in% env.drops),],drop=TRUE)
colnames(env.df)=c("Date_Sampled",
                   "Substrate",
                   "NEP",
                   "GPP",
                   "Respiration",
                   "BG-ase",
                   "NAG-ase",
                   "P-ase",
                   "Chl.a",
                   "Biomass",
                   "Day of Incubation",
                   "SRP",
                   "NO3",
                   "NH4",
                   "doy",
                   "width",
                   "xdepth",
                   "xvelocity",
                   "xtemp",
                   "xspc",
                   "Q")

env.c.drops=c("X",  #I don't necessarily want to use all the data here, so I specified some columns to ignore (summary columns, duplicates, etc)
              "Date.Sampled",
              "Sample",
              "time",
              "doy",
              "width",
              "xdepth",
              "xvelocity",
              "xtemp",
              "xspc",
              "Q")


env.df.fit=env.df[ , !(names(env.df) %in% env.c.drops)] #this si the code to remove the specified columns

its.df.3=subset.data.frame(its.df, taxlevel==3,drop=TRUE) %>% 
  select(-taxlevel,-rankID,-daughterlevels,-total,-SDKC2,-SDKC3)

its.df.sp<-its.df.3  %>% 
  gather(key="Sample",value="obs",-taxon) %>% 
  separate(Sample, into=c("Junk","Date","Substrate","Rep"),sep=c(1,7,-1))

its.df.sp$Junk=NULL
its.df.sp=droplevels(its.df.sp[-which(its.df.sp$Substrate=='CR'),])
env.df$Date_Sampled=as.factor(env.df$Date_Sampled)
env.df$Date_Sampled=revalue(env.df$Date_Sampled,c("8/18/2017"="3","8/29/2017"="10","9/5/2017"="21","9/15/2017"="31"))
env.df$Substrate=factor(env.df$Substrate,levels=c("H","So","Sh","F","T"),
                           labels=c("PVC","Soft PE","Sheet PE","PS","Tile"))


end.df.names=c("Date_Sampled","Substrate")
env.df.fac=env.df[,names(env.df) %in% end.df.names]
 #god thats a lot fo data munging...I know I've done this in toher chunks but I want each chunk to stand on its own if need be.

its.df.sps<-its.df.sp %>% 
  spread(key=taxon, value=obs) %>% 
  dplyr::select(-Date,-Substrate,-Rep)
#spread take a tall data table and makes it wide.  each "key" value becomes a column populated by the "value"
  


its.ord.dist=vegdist(its.df.sps) #get distances
its.ord.nmds=metaMDS(its.ord.dist) #run nmds
its.ORD=data.frame(MDS1=its.ord.nmds$points[,1],MDS2=its.ord.nmds$points[,2]) #exrtact nmds coordinates

#its.ORD=merge(its.ORD, env.df.fac, by="Date")
its.ORD=cbind(its.ORD,env.df.fac) #combine nmds coordinates with env data

its.fit=envfit(its.ord.nmds,env.df.fit,perm=999,na.rm=T,display="sites") #correlate you nmds coordintes with env data  Higher perm number means mroe sensitive p value

its.fit.df=as.data.frame(its.fit$vectors$arrows*sqrt(its.fit$vectors$r)) #exracting the data to plot the vectors
its.fit.df$species=rownames(its.fit.df) #converting rownmaes (inaccesible data) to a column (accessible data)
its.spp.scrs=as.data.frame(scores(its.fit,display="vectors"))
its.spp.scrs=cbind(its.spp.scrs, Assay=rownames(its.spp.scrs),its.fit$vectors$pvals) #Combining vector coordinates, pvalues etc into a data table for plotting 
its.spp.scrs=setnames(its.spp.scrs, old=("its.fit$vectors$pvals"), new=c("pvals")) #Changing the name for clarity's sake
its.spp.scrs=its.spp.scrs[ which (its.spp.scrs$pvals<0.05),] #Here we select only the  vectors whose p.vals are less than 0.05.


its.nmds=ggplot(its.ORD)+
  geom_point(mapping= aes(MDS1,MDS2,shape=Substrate,color=Date_Sampled,fill=Date_Sampled))+
  coord_fixed(ratio=1)+
  geom_segment(data=its.spp.scrs, 
               aes(x=0, xend=its.spp.scrs$NMDS1/2, 
                   y=0,yend=its.spp.scrs$NMDS2/2),
               arrow=arrow(length=unit(0.1, "cm")),
               color="red")+
  geom_text_repel(data = its.spp.scrs, 
            aes(x = NMDS1/2, 
                y = NMDS2/2, label = Assay),
            size = 3) +
  ggtitle("Fungal Communities")+
  scale_fill_viridis_d("Day of Incubation")+
  scale_color_viridis_d("Day of Incubation")+
  theme_classic()+
  labs(shape="Date Sampled", color="Substrate")+
  xlim(-0.5,0.5)+
  ylim(-0.5,0.5)+
  scale_shape_manual(name="Substrate",values=c(21,22,23,24,25)) #I adjuested the length of the arrows (NMDS1/5, etc) so they woudl fit better int he plot.  This is purely for aesthtetic pruposes
its.nmds


ggplot(its.ORD)+
  geom_point(mapping= aes(MDS1,MDS2,shape=Date_Sampled,color=Substrate,group=Substrate))+facet_grid(.~Date_Sampled)

ggplot(its.ORD)+
  geom_point(mapping= aes(MDS1,MDS2,shape=Date_Sampled,color=Substrate,group=Substrate))+facet_grid(.~Substrate)


#This section should really be its own chunk.  betadisper compares the area of the collected nmds points for each group you specifiy and compares it to your other group areas (a la anova)

#http://cc.oulu.fi/~jarioksa/softhelp/vegan/html/betadisper.html
disp.Sample=betadisper(its.ord.dist,its.ORD$Substrate)
boxplot(disp.Sample)
plot(disp.Sample)
disp.Date=betadisper(its.ord.dist,its.ORD$Date_Sampled)
boxplot(disp.Date)
plot(disp.Date)
perm.date=permutest(disp.Date, pairwise=TRUE,permutations=1000) 
TukeyHSD(disp.Sample) #compaitble with tukeyhsd for pairwise comparions!
perm.sample=permutest(disp.Sample, pairwise=TRUE,permutations=1000)
TukeyHSD(disp.Date)
disp.Sample
disp.Date
#test pairwise comparions between groups in nmds



####

  





```

```{r Venn Diagrams,echo=FALSE}
#extract and shape OTU table at Order level
its.df.venn=subset.data.frame(its.df, taxlevel==4,drop=TRUE)

drops <- c("taxlevel","rankID","daughterlevels","total","SDKC1","SDKC2","SDKC3")
its.df.venn=its.df.venn[ , !(names(its.df.venn) %in% drops)]

its.df.venn=melt(its.df.venn,id.vars="taxon")
its.df.venn=dcast(its.df.venn,variable~its.df.venn$taxon)
#rabund.df$variable=as.numeric(rabund.df$variable)
its.df.venn=separate(its.df.venn,variable,into=c("junk","Date","Substrate"),sep=c(1,7))
its.df.venn=separate(its.df.venn,Substrate,into=c("Substrate","Rep"),sep=c(-1))
its.df.venn$junk=NULL

its.df.venn=droplevels(its.df.venn[-which(its.df.venn$Substrate=='CR'),])
its.df.venn$Date=as.factor(its.df.venn$Date)
its.df.venn$Date=revalue(its.df.venn$Date,c("170818"="3","170828"="10","170905"="21","170915"="31"))

Hard=colnames(its.df.venn[its.df.venn$Substrate=="H",apply(its.df.venn[its.df.venn$Substrate=="H",],MARGIN=2,function(x) any(x>0))])
Hard=Hard[-(1:3)]

Soft=colnames(its.df.venn[its.df.venn$Substrate=="SO",apply(its.df.venn[its.df.venn$Substrate=="SO",],MARGIN=2,function(x) any(x>0))])
Soft=Soft[-(1:3)]

Sheet=colnames(its.df.venn[its.df.venn$Substrate=="SH",apply(its.df.venn[its.df.venn$Substrate=="SH",],MARGIN=2,function(x) any(x>0))])
Sheet=Sheet[-(1:3)]

Foam=colnames(its.df.venn[its.df.venn$Substrate=="F",apply(its.df.venn[its.df.venn$Substrate=="F",],MARGIN=2,function(x) any(x>0))])
Foam=Foam[-(1:3)]

Tile=colnames(its.df.venn[its.df.venn$Substrate=="T",apply(its.df.venn[its.df.venn$Substrate=="T",],MARGIN=2,function(x) any(x>0))])
Tile=Tile[-(1:3)]
 its.venn.list=list("Hard"=Hard,"Soft"=Soft,"Sheet"=Sheet,"Foam"=Foam,"Tile"=Tile)
#its.venn=gList(group.venn(list("Hard"=Hard,"Soft"=Soft,"Sheet"=Sheet,"Foam"=Foam,"Tile"=Tile),label=TRUE))
its.venn=gList(venn.diagram(x=its.venn.list,filename=NULL,
                            label=TRUE,main="Fungal Communities",main.fontface = 2,
                            fill=c("yellow","blue","green","red","purple"),
                            cat.cex=0.8,cat.fontface=2,lty=1,main.pos=c(0.15,1),
                            cat.just=list(c(0.6,1) , c(0,0) , c(0,0) , c(1,1) ,c(1,0))))

grid.arrange(gTree(children=its.venn))
#its.venn

```


```{r Venn Time,echo=FALSE}
#extract and shape OTU table at Order level
its.df.venn=subset.data.frame(its.df, taxlevel==5,drop=TRUE)

drops <- c("taxlevel","rankID","daughterlevels","total","SDKC1","SDKC2","SDKC3")
its.df.venn=its.df.venn[ , !(names(its.df.venn) %in% drops)]

its.df.venn=melt(its.df.venn,id.vars="taxon")
its.df.venn=dcast(its.df.venn,variable~its.df.venn$taxon)
#rabund.df$variable=as.numeric(rabund.df$variable)
its.df.venn=separate(its.df.venn,variable,into=c("junk","Date","Substrate"),sep=c(1,7))
its.df.venn=separate(its.df.venn,Substrate,into=c("Substrate","Rep"),sep=c(-1))
its.df.venn$junk=NULL

its.df.venn=droplevels(its.df.venn[-which(its.df.venn$Substrate=='CR'),])
its.df.venn$Date=as.factor(its.df.venn$Date)
its.df.venn$Date=revalue(its.df.venn$Date,c("170818"="3","170828"="10","170905"="21","170915"="31"))

Day_3=colnames(its.df.venn[its.df.venn$Date=="3",apply(its.df.venn[its.df.venn$Date=="3",],MARGIN=2,function(x) any(x>0))])

Day_10=colnames(its.df.venn[its.df.venn$Date=="10",apply(its.df.venn[its.df.venn$Date=="10",],MARGIN=2,function(x) any(x>0))])

Day_21=colnames(its.df.venn[its.df.venn$Date=="21",apply(its.df.venn[its.df.venn$Date=="21",],MARGIN=2,function(x) any(x>0))])

Day_31=colnames(its.df.venn[its.df.venn$Date=="31",apply(its.df.venn[its.df.venn$Date=="31",],MARGIN=2,function(x) any(x>0))])

venn(list("Day 3"=Day_3,"Day 10"=Day_10,"Day 21"=Day_21,"Day 31"=Day_31))
```


```{r}
saveRDS(its.df,"fungus.Rdata")
```

